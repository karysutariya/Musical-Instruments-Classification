{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:33:19.911750: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-05 18:33:19.912938: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 18:33:19.936711: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 18:33:19.937314: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 18:33:20.303536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "#from scripts\n",
    "from Attention import *\n",
    "from arguments import parse_arguments \n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from Attention import *\n",
    "from data_utils import *\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2638689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing your install of VGGish\n",
      "\n",
      "Resampling via resampy works!\n",
      "Log Mel Spectrogram example:  [[-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]\n",
      " [-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]\n",
      " [-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]\n",
      " ...\n",
      " [-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]\n",
      " [-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]\n",
      " [-4.48313252 -4.27083405 -4.17064267 ... -4.60069383 -4.60098887\n",
      "  -4.60116305]]\n",
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:33:22.080910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 18:33:22.081256: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/kary/anaconda3/envs/musical/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/home/kary/anaconda3/envs/musical/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:325: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "2023-10-05 18:33:22.253659: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish embedding:  [-2.72986382e-01 -1.80314243e-01  5.19921482e-02 -1.43571466e-01\n",
      " -1.04673848e-01 -4.96598154e-01 -1.75267860e-01  4.23147976e-01\n",
      " -8.22126091e-01 -2.16801703e-01 -1.17509484e-01 -6.70077085e-01\n",
      "  1.43174529e-01 -1.44183964e-01  8.73494893e-03 -8.71973336e-02\n",
      " -1.84393510e-01  5.96655548e-01 -3.43809605e-01 -5.79106510e-02\n",
      " -1.65071234e-01  4.22911346e-02 -2.55293489e-01 -2.36356825e-01\n",
      "  1.80295452e-01  3.02612156e-01  1.08356804e-01 -4.48397934e-01\n",
      "  1.22757599e-01 -2.99955249e-01 -5.55934012e-01  5.05966663e-01\n",
      "  2.05210567e-01  8.87592018e-01  9.03702617e-01 -2.10566297e-01\n",
      " -3.27461362e-02  1.38691485e-01 -2.27416620e-01  1.14804089e-01\n",
      "  5.95409870e-01 -4.76971269e-01  2.28232607e-01  1.54626817e-01\n",
      "  1.64934054e-01  7.19253302e-01  1.24101830e+00  5.61996639e-01\n",
      "  2.73531914e-01  3.09790373e-02  2.10977703e-01 -6.09551966e-01\n",
      " -3.15282673e-01  1.76392615e-01 -8.96194577e-02 -4.26822454e-01\n",
      "  3.12993884e-01 -1.56592414e-01  3.31673682e-01  1.29436329e-01\n",
      "  1.66024074e-01  3.01902294e-02 -1.54465020e-01 -4.29332376e-01\n",
      " -2.68703669e-01 -1.58071116e-01  4.00485456e-01 -2.55945206e-01\n",
      " -2.66428739e-02  8.16178322e-03  2.98492700e-01  3.48756194e-01\n",
      " -1.07143685e-01  8.88779685e-02  1.26810461e-01 -3.34817052e-01\n",
      " -2.55427867e-01  5.07779479e-01  3.97584677e-01  1.78759396e-01\n",
      " -8.04519132e-02  4.84319329e-02 -2.01263130e-01 -2.97957659e-01\n",
      "  3.66831303e-01  4.56224501e-01  5.37960827e-01 -2.00489163e-02\n",
      " -6.24544770e-02  4.15623128e-01 -1.88741416e-01 -5.36903262e-01\n",
      " -1.78362101e-01  3.81367207e-01  3.96644890e-01  3.21936667e-01\n",
      " -4.26685996e-02 -1.41018227e-01 -4.53833640e-01 -1.07017368e-01\n",
      " -2.21892685e-01  3.51183236e-01 -2.58386433e-01  3.31110179e-01\n",
      " -7.28938937e-01 -2.55487442e-01  3.56360793e-01 -3.16188395e-01\n",
      "  3.12793612e-01  1.23501725e-01 -1.83649994e-02 -3.99396032e-01\n",
      " -5.13507426e-01 -2.74227262e-01 -2.68650711e-01  2.24091411e-01\n",
      "  1.09624743e-01  1.30929992e-01 -1.25994980e-01 -1.92614928e-01\n",
      "  1.83567405e-04  2.04150319e-01 -1.03096768e-01  2.93377936e-02\n",
      " -3.38305771e-01 -2.25750118e-01 -2.46723339e-01 -1.20763391e-01]\n",
      "embedding mean/stddev 0.000656981 0.34301957\n",
      "Postprocessed VGGish embedding:  [160  53 124 132 154 120 119 105 155 173 129  69 149  93  59   0  52  97\n",
      " 157 144 153 194 251 108  48 174 131 190 195  79  59  60 169  93 167 247\n",
      "  28  75 255  56 134 169 234 137 232 100  19  80 162 255   0 255 101   0\n",
      " 222 252  79 211  64  88 248   0   0 255 246  62  81 255   0 159  22 168\n",
      "  70 255  99 135 204 192 255 150   0   0 255 255  67 235  55 255  69   0\n",
      "   0  17 241  44 255 224   0 255  40   0 255   0 211 252  62   0  28 218\n",
      " 112   0 255   0  81  67 153   0 255   0 129 229  53 255  55 101   0 255\n",
      "   0 255]\n",
      "postproc embedding mean/stddev 126.359375 89.33878063086252\n",
      "\n",
      "Looks Good To Me!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vggish_smoke_test import *\n",
    "# import vggish_slim\n",
    "# import vggish_params\n",
    "# import vggish_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
    "  \"\"\"Define VGGish model, load the checkpoint, and return a dictionary that points\n",
    "  to the different tensors defined by the model.\n",
    "  \"\"\"\n",
    "  vggish_slim.define_vggish_slim()\n",
    "  checkpoint_path = 'vggish_model.ckpt'\n",
    "  vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
    "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "  features_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.INPUT_TENSOR_NAME)\n",
    "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.OUTPUT_TENSOR_NAME)\n",
    "  layers = {'conv1': 'vggish/conv1/Relu',\n",
    "            'pool1': 'vggish/pool1/MaxPool',\n",
    "            'conv2': 'vggish/conv2/Relu',\n",
    "            'pool2': 'vggish/pool2/MaxPool',\n",
    "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
    "            'pool3': 'vggish/pool3/MaxPool',\n",
    "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
    "            'pool4': 'vggish/pool4/MaxPool',\n",
    "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
    "            #'fc2': 'vggish/fc2/Relu',\n",
    "            'embedding': 'vggish/embedding',\n",
    "            'features': 'vggish/input_features',\n",
    "         }\n",
    "  g = tf.get_default_graph()\n",
    "  for k in layers:\n",
    "    layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
    "  return {'features': features_tensor,\n",
    "          'embedding': embedding_tensor,\n",
    "          'layers': layers,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ProcessWithVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
    "  floats between -1 and +1.'''\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "  [embedding_batch] = sess.run([vgg['embedding']],\n",
    "                               feed_dict={vgg['features']: input_batch})\n",
    "  # Postprocess the results to produce whitened quantized embeddings.\n",
    "  pca_params_path = 'vggish_pca_params.npz'\n",
    "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
    "  return postprocessed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "vgg = CreateVGGishNetwork(0.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v, sr_v  = librosa.load('Demon-Slayer.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v = _ProcessWithVGGish(vgg, y_v, sr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_v1, k_v2 = X_v.shape\n",
    "X_v = np.asarray(X_v).reshape(-1,k_v1,k_v2)\n",
    "X_tst_torch_v = (torch.from_numpy(X_v)).type(torch.float32)\n",
    "X_tst_torch_v =(X_tst_torch_v/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #'cuda' if torch.cuda.is_available() else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55336\n"
     ]
    }
   ],
   "source": [
    "modelPath = 'log/Attention/Attention/trainedModel_0.pth'\n",
    "\n",
    "model = DecisionLevelSingleAttention(\n",
    "                freq_bins=128,\n",
    "                classes_num=20,\n",
    "                emb_layers=3,\n",
    "                hidden_units=128,\n",
    "                drop_rate=0.6)\n",
    "\n",
    "\n",
    "#Restore model\n",
    "model.load_state_dict(torch.load(modelPath))\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_v = model(X_tst_torch_v)\n",
    "\n",
    "output_v = to_numpy(output_v).ravel()\n",
    "# print(output_v.shape)\n",
    "# print(output_v) \n",
    "outputIndex = np.where(output_v >= 0.9)\n",
    "# print(outputIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classMap= {'accordion': 0,\n",
    " 'banjo': 1,\n",
    " 'bass': 2,\n",
    " 'cello': 3,\n",
    " 'clarinet': 4,\n",
    " 'cymbals': 5,\n",
    " 'drums': 6,\n",
    " 'flute': 7,\n",
    " 'guitar': 8,\n",
    " 'mallet_percussion': 9,\n",
    " 'mandolin': 10,\n",
    " 'organ': 11,\n",
    " 'piano': 12,\n",
    " 'saxophone': 13,\n",
    " 'synthesizer': 14,\n",
    " 'trombone': 15,\n",
    " 'trumpet': 16,\n",
    " 'ukulele': 17,\n",
    " 'violin': 18,\n",
    " 'voice': 19}\n",
    "inv_map = {v: k for k, v in classMap.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of detected musical instruments: \n",
      "1. saxophone\n",
      "2. trombone\n",
      "3. trumpet\n",
      "4. voice\n"
     ]
    }
   ],
   "source": [
    "print(\"List of detected musical instruments: \")\n",
    "for j,i in enumerate(outputIndex[0]):\n",
    "    print(str(j+1)+\". \"+inv_map[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accordion 0.06983575\n",
    "# banjo 0.33101374\n",
    "# bass 0.020017007\n",
    "# cello 0.06161041\n",
    "# clarinet 0.12218426\n",
    "# cymbals 0.0016985144\n",
    "# drums 0.014519502\n",
    "# flute 0.700888\n",
    "# guitar 0.8985557\n",
    "# mallet_percussion 0.5362431\n",
    "# mandolin 0.59339917\n",
    "# organ 0.045906793\n",
    "# piano 0.7231139\n",
    "# saxophone 0.0318367\n",
    "# synthesizer 0.23019123\n",
    "# trombone 0.010454232\n",
    "# trumpet 0.025121506\n",
    "# ukulele 0.4342751\n",
    "# violin 0.25603348\n",
    "# voice 0.0148326345\n",
    "\n",
    "# flute, guitar,mallet_percussion, piano, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
